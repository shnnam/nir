{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SC3IdFKs8pCd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Siren\n",
    "from util import get_mgrid, jacobian, VideoFitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reflection(path, total_steps, lambda_interf=0.02, lambda_flow=0.02, lambda_excl=0.0005, verbose=True, steps_til_summary=100):\n",
    "    g = Siren(in_features=3, out_features=2, hidden_features=256,\n",
    "              hidden_layers=4, outermost_linear=True)\n",
    "    g.cuda()\n",
    "    f1 = Siren(in_features=2, out_features=3, hidden_features=256,\n",
    "               hidden_layers=4, outermost_linear=True)\n",
    "    f1.cuda()\n",
    "    f2 = Siren(in_features=3, out_features=3, hidden_features=256, \n",
    "               hidden_layers=4, outermost_linear=True)\n",
    "    f2.cuda()\n",
    "\n",
    "    optim = torch.optim.Adam(lr=1e-4, params=chain(g.parameters(), f1.parameters(), f2.parameters()))\n",
    "\n",
    "    v = VideoFitting(path)\n",
    "    videoloader = DataLoader(v, batch_size=1, pin_memory=True, num_workers=0)\n",
    "    model_input, ground_truth = next(iter(videoloader))\n",
    "    model_input, ground_truth = model_input[0].cuda(), ground_truth[0].cuda()\n",
    "\n",
    "    batch_size = (v.H * v.W) // 8\n",
    "    for step in range(total_steps):\n",
    "        start = (step * batch_size) % len(model_input)\n",
    "        end = min(start + batch_size, len(model_input))\n",
    "\n",
    "        xyt = model_input[start:end].requires_grad_()\n",
    "        xy, t = xyt[:, :-1], xyt[:, [-1]]\n",
    "        h = g(xyt)\n",
    "        xy_ = xy + h\n",
    "        o_scene = torch.sigmoid(f1(xy_))\n",
    "        o_obst = torch.sigmoid(f2(torch.cat((xy, t), -1)))\n",
    "        o = o_scene + o_obst\n",
    "        loss_recon = ((o - ground_truth[start:end]) ** 2).mean()\n",
    "        loss_interf = o_obst.abs().mean()\n",
    "        loss_flow = jacobian(h, xyt).abs().mean()\n",
    "\n",
    "        g_scene = jacobian(o_scene, xy_)\n",
    "        g_obst = jacobian(o_obst, xy)\n",
    "        n_scene = (g_obst.norm(dim=0, keepdim=True) / g_scene.norm(dim=0, keepdim=True)).sqrt()\n",
    "        n_obst = (g_scene.norm(dim=0, keepdim=True) / g_obst.norm(dim=0, keepdim=True)).sqrt()\n",
    "        loss_excl = (torch.tanh(n_scene * g_scene) * torch.tanh(n_obst * g_obst)).pow(2).mean()\n",
    "\n",
    "        loss = loss_recon + lambda_interf * loss_interf + lambda_flow * loss_flow + lambda_excl * loss_excl\n",
    "\n",
    "        if not step % steps_til_summary:\n",
    "            print(\"Step [%04d/%04d]: recon=%0.8f, interf=%0.4f, flow=%0.4f, excl=%0.4f\" % (step, total_steps, loss_recon, loss_interf, loss_flow, loss_excl))\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    return g, f1, f2, v.video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, f1, f2, orig = train_reflection('./data/reflection', 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    N, _, H, W = orig.size()\n",
    "    xyt = get_mgrid([H, W, N]).cuda()\n",
    "    h = g(xyt)\n",
    "    o_scene = torch.sigmoid(f1(xyt[:, :-1] + h))\n",
    "    o_obst = torch.sigmoid(f2(xyt))\n",
    "    o_scene = o_scene.view(H, W, N, 3).permute(2, 0, 1, 3).cpu().detach().numpy()\n",
    "    o_obst = o_obst.view(H, W, N, 3).permute(2, 0, 1, 3).cpu().detach().numpy()\n",
    "    o_scene = (o_scene * 255).astype(np.uint8)\n",
    "    o_obst = (o_obst * 255).astype(np.uint8)\n",
    "    o_scene = [o_scene[i] for i in range(len(o_scene))]\n",
    "    o_obst = [o_obst[i] for i in range(len(o_obst))]\n",
    "    orig = orig.permute(0, 2, 3, 1).detach().numpy()\n",
    "    orig = (orig * 255).astype(np.uint8)\n",
    "    orig = [orig[i] for i in range(len(orig))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out video\n",
    "# ! pip install --user imageio imageio-ffmpeg\n",
    "import imageio\n",
    "fn_orig = os.path.join('./data/reflecrtion_orig.mp4')\n",
    "fn_scene = os.path.join('./data/reflection_scene.mp4')\n",
    "fn_obst = os.path.join('./data/reflection_interf.mp4')\n",
    "imageio.mimwrite(fn_orig, orig, fps=1)\n",
    "imageio.mimwrite(fn_scene, o_scene, fps=1)\n",
    "imageio.mimwrite(fn_obst, o_obst, fps=1)\n",
    "\n",
    "# Display video inline\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "data_url_orig = \"data:video/mp4;base64,\" + b64encode(open(fn_orig, 'rb').read()).decode()\n",
    "data_url_scene = \"data:video/mp4;base64,\" + b64encode(open(fn_scene, 'rb').read()).decode()\n",
    "data_url_obst = \"data:video/mp4;base64,\" + b64encode(open(fn_obst, 'rb').read()).decode()\n",
    "HTML(f'''\n",
    "<video width=384 controls autoplay loop>\n",
    "      <source src=\"{data_url_orig}\" type=\"video/mp4\">\n",
    "</video>\n",
    "<video width=384 controls autoplay loop>\n",
    "      <source src=\"{data_url_scene}\" type=\"video/mp4\">\n",
    "</video>\n",
    "<video width=384 controls autoplay loop>\n",
    "      <source src=\"{data_url_obst}\" type=\"video/mp4\">\n",
    "</video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fence(path, total_steps, lambda_interf=0.001, lambda_flow=0.02, verbose=True, steps_til_summary=100):\n",
    "    g = Siren(in_features=3, out_features=2, hidden_features=256,\n",
    "              hidden_layers=4, outermost_linear=True)\n",
    "    g.cuda()\n",
    "    f1 = Siren(in_features=2, out_features=3, hidden_features=256,\n",
    "               hidden_layers=4, outermost_linear=True, first_omega_0=90.)\n",
    "    f1.cuda()\n",
    "    f2 = Siren(in_features=3, out_features=4, hidden_features=256, \n",
    "               hidden_layers=4, outermost_linear=True)\n",
    "    f2.cuda()\n",
    "\n",
    "    optim = torch.optim.Adam(lr=1e-4, params=chain(g.parameters(), f1.parameters(), f2.parameters()))\n",
    "\n",
    "    v = VideoFitting(path)\n",
    "    videoloader = DataLoader(v, batch_size=1, pin_memory=True, num_workers=0)\n",
    "    model_input, ground_truth = next(iter(videoloader))\n",
    "    model_input, ground_truth = model_input[0].cuda(), ground_truth[0].cuda()\n",
    "\n",
    "    batch_size = (v.H * v.W) // 8\n",
    "    for step in range(total_steps):\n",
    "        start = (step * batch_size) % len(model_input)\n",
    "        end = min(start + batch_size, len(model_input))\n",
    "\n",
    "        xyt = model_input[start:end].requires_grad_()\n",
    "        xy, t = xyt[:, :-1], xyt[:, [-1]]\n",
    "        h = g(xyt)\n",
    "        xy_ = xy + h\n",
    "        o_scene = torch.sigmoid(f1(xy_))\n",
    "        o_obst = torch.sigmoid(f2(xyt))\n",
    "        o_obst, alpha = o_obst[:, :-1], o_obst[:, [-1]]\n",
    "        o = (1 - alpha) * o_scene + alpha * o_obst\n",
    "        loss_recon = ((o - ground_truth[start:end]) ** 2).mean()\n",
    "        loss_interf = alpha.abs().mean()\n",
    "        loss_flow = jacobian(h, xyt).abs().mean()\n",
    "        loss = loss_recon + lambda_interf * loss_interf + lambda_flow * loss_flow\n",
    "\n",
    "        if not step % steps_til_summary:\n",
    "            print(\"Step [%04d/%04d]: recon=%0.8f, interf=%0.4f, flow=%0.4f\" % (step, total_steps, loss_recon, loss_interf, loss_flow))\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    return g, f1, f2, v.video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, f1, f2, orig = train_fence('./data/fence', 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    N, _, H, W = orig.size()\n",
    "    xyt = get_mgrid([H, W, N]).cuda()\n",
    "    h = g(xyt)\n",
    "    o_scene = torch.sigmoid(f1(xyt[:, :-1] + h))\n",
    "    o_obst = torch.sigmoid(f2(xyt))\n",
    "    o_obst = o_obst[:, :-1] * o_obst[:, [-1]]\n",
    "    o_scene = o_scene.view(H, W, N, 3).permute(2, 0, 1, 3).cpu().detach().numpy()\n",
    "    o_obst = o_obst.view(H, W, N, 3).permute(2, 0, 1, 3).cpu().detach().numpy()\n",
    "    o_scene = (o_scene * 255).astype(np.uint8)\n",
    "    o_obst = (o_obst * 255).astype(np.uint8)\n",
    "    o_scene = [o_scene[i] for i in range(len(o_scene))]\n",
    "    o_obst = [o_obst[i] for i in range(len(o_obst))]\n",
    "    orig = orig.permute(0, 2, 3, 1).detach().numpy()\n",
    "    orig = (orig * 255).astype(np.uint8)\n",
    "    orig = [orig[i] for i in range(len(orig))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save out video\n",
    "# ! pip install --user imageio imageio-ffmpeg\n",
    "import imageio\n",
    "fn_orig = os.path.join('./data/fence_orig.mp4')\n",
    "fn_scene = os.path.join('./data/fence_scene.mp4')\n",
    "fn_obst = os.path.join('./data/fence_interf.mp4')\n",
    "imageio.mimwrite(fn_orig, orig, fps=1)\n",
    "imageio.mimwrite(fn_scene, o_scene, fps=1)\n",
    "imageio.mimwrite(fn_obst, o_obst, fps=1)\n",
    "\n",
    "# Display video inline\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "data_url_orig = \"data:video/mp4;base64,\" + b64encode(open(fn_orig, 'rb').read()).decode()\n",
    "data_url_scene = \"data:video/mp4;base64,\" + b64encode(open(fn_scene, 'rb').read()).decode()\n",
    "data_url_obst = \"data:video/mp4;base64,\" + b64encode(open(fn_obst, 'rb').read()).decode()\n",
    "HTML(f'''\n",
    "<video width=512 controls autoplay loop>\n",
    "      <source src=\"{data_url_orig}\" type=\"video/mp4\">\n",
    "</video>\n",
    "<video width=512 controls autoplay loop>\n",
    "      <source src=\"{data_url_scene}\" type=\"video/mp4\">\n",
    "</video>\n",
    "<video width=512 controls autoplay loop>\n",
    "      <source src=\"{data_url_obst}\" type=\"video/mp4\">\n",
    "</video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "explore_siren.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
